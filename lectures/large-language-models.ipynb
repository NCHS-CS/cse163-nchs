{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Models\n",
    "\n",
    "In this lesson, we'll learn about the newest advances in **large language models** (LLMs) since the advent of the transformer architecture. By the end of this lesson, students will be able to:\n",
    "\n",
    "- Explain the conceptual ideas behind large language models.\n",
    "- Explain techniques for improving large language models such as fine-tuning and reinforcement learning from human feedback.\n",
    "- Identify the relationship between model parameters on model performance and environmental as well as compute costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do LLMs work?\n",
    "\n",
    "Below are two videos from Prof. Steve Seitz's YouTube Channel [\"Graphics in 5 Minutes\"](https://www.youtube.com/@g5min). [GPT in 60 Lines of Numpy](https://jaykmody.com/blog/gpt-from-scratch/) by Jay Mody shows how one can replicate (the inference half of) a tiny verion of GPT-2 in just 60 lines of Python. We can try this code out in [Colab](https://colab.research.google.com/drive/1YWxWKWRZdRTW-TmBpf4xe9byqpSd6_6L?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/lnA9DMvHtfI?si=QJRk0fEHZNbuKf8b\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/YDiSFS-yHwk?si=KY34lWBCaoIzNCEW\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques for Improving LLMs\n",
    "\n",
    "Now we look at techniques that help better train LLMs or improve the performance during inference.\n",
    "\n",
    "### Instruction fine-tuning\n",
    "\n",
    "Fine-tuning in the world of neural networks is to take an already trained network, and use another dataset to run through some training iterations again to update that trained network's parameters so that the fine-tuned version can perform better on a task related to that second dataset. Similar ideas work well for transformers too.\n",
    "\n",
    "[Fine-tune Gemma models in Keras using LoRA (colab notebook)](https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lora_tuning.ipynb?utm_source=agd&utm_medium=referral&utm_campaign=open-in-colab&hl=de) or the [doc version](https://ai.google.dev/gemma/docs/lora_tuning)\n",
    "\n",
    "### Reinforcement learning from human feedback\n",
    "\n",
    "Reinforcement learning refers to a kind of machine learning where a policy is learned so that an agent will follow this policy to take an action to respond to different states in order to achieve the maximum accumulated reward. The reinforcement aspect basically refers to the fact that the agent behavior is \"guided\" by the reward, and rewards in the desired behavior reinforces the agent's learned policy so that it will tend to choose actions that lead to greater rewards. For LLM agents, human-in-the-loop feedback is also very helpful.\n",
    "\n",
    "[RLHF Wikipedia page](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback)\n",
    "\n",
    "### Few-shot learning\n",
    "\n",
    "Unlike the previous two techniques that improve LLM training, few-shot learning does not change model parameters but simply employ interesting prompting strategies by providing examples in the prompt so that the LLM can learn through its context window.\n",
    "\n",
    "[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\n",
    "\n",
    "> While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches.\n",
    "\n",
    "### Chain-of-thought prompting\n",
    "\n",
    "Another interesting strategy is to include all the intermediate steps, which is a kind of few-show learning but brings it one step further.\n",
    "\n",
    "[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)\n",
    "\n",
    "> We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.\n",
    "\n",
    "[Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601)\n",
    "\n",
    "> ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model Parameters\n",
    "\n",
    "[Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)\n",
    "\n",
    "> By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled.\n",
    "\n",
    "[Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)\n",
    "\n",
    "> Making language models bigger does not inherently make them better at following a user's intent... In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters.... Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.\n",
    "\n",
    "[Energy and Policy Considerations for Deep Learning in NLP](https://arxiv.org/abs/1906.02243)\n",
    "\n",
    "<table><thead><tr><th></th><th>Date of original paper</th><th>Energy consumption (kWh)</th><th>Carbon footprint (lbs of CO<sub>2</sub>e)</th><th>Cloud compute cost (USD)</th></tr></thead><tbody><tr><td>GPT-2</td><td>Feb, 2019</td><td></td><td></td><td>\\$12,902â€“\\$43,008</td></tr><tr><td>Transformer (213M parameters)</td><td>Jun, 2017</td><td>201</td><td>192</td><td>\\$289â€“\\$981</td></tr><tr><td>BERT (110M parameters)</td><td>Oct, 2018</td><td>1,507</td><td>1,438</td><td>\\$3,751â€“\\$12,571</td></tr><tr><td>Transformer (65M parameters)</td><td>Jun, 2017</td><td>27</td><td>26</td><td>\\$41â€“\\$140</td></tr><tr><td>ELMo</td><td>Feb, 2018</td><td>275</td><td>262</td><td>\\$433â€“\\$1,472</td></tr><tr><td>Transformer (213M parameters) w/ neural architecture search</td><td>Jan, 2019</td><td>656,347</td><td>626,155</td><td>\\$942,973â€“\\$3,201,722</td></tr></tbody></table>\n",
    "\n",
    "[On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ](https://dl.acm.org/doi/10.1145/3442188.3445922)\n",
    "\n",
    "> In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
